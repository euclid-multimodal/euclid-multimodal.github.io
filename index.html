<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions">
  <meta name="keywords" content="MLLM, Foundation Models, Low-Level Visual Perception, Geometric Perception, Synthetic Data">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions</title>
  <style>
      /* Define a reusable class for the gradient */
      .gradient-euclid {
          background: linear-gradient(to right, rgb(65, 105, 225), rgb(128, 0, 128));
          -webkit-background-clip: text;
          background-clip: text;
          -webkit-text-fill-color: transparent;
      }
  </style>

<!-- Google Tag Manager -->
<!-- <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script> -->
<!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/tifa.png"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
  <!-- Google Tag Manager (noscript) -->
  <!-- <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript> -->
  <!-- End Google Tag Manager (noscript) -->

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/abs/2305.00586">
            How does GPT-2 compute greater-than?
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2310.17086">
            How Transformers learn in-context?
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2310.07972">
            How to interpret diffusion models?
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2311.17946">
            How to improve text-to-image alignment?
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2312.06550">
            LLM360: Towards Fully Transparent Open-Source LLMs
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="gradient-euclid"><b>Euclid</b></span>: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://saccharomycetes.github.io/">Jiarui Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ollieliu.com/">Ollie Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=e-FRHr4AAAAJ&hl=zh-TW">Tianyu Yu</a><sup>2</sup>,
            </span>
            <span class="author-block">
                <a href="https://jameshujy.github.io/">Jinyi Hu</a><sup>2</sup>,
              </span>
            <span class="author-block">
              <a href="https://willieneis.github.io/">Willie Neiswanger</a><sup>1</sup>
            </span>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1 </sup>University of Southern California</span>
            <span class="author-block"><sup>2 </sup>Tsinghua University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.02392"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/EuclidAI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">ðŸ¤—</span>
                  <span>Model</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/euclid-multimodal/Euclid"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered" style="max-width:870px;">
        Classical decision theory has helped humans make rational decisions for decades. Can it do the same for Large Language Models (LLMs)?
        <span class="dnerf" style="color: rgb(153, 27, 30);">DeLLMa</span> (pronouced dilemma) is a Decision-making Large Language Model assistant, which utilizes the power of <span style="color: red;"><b>Large Language Models</b></span> and <span style="color: red;"><b>Classical Decision Theory</b></span> to help LLMs make decisions under uncertainty.
      </h2>
      <div style="text-align: center;"><br/>
      <img src="./static/images/dellma-overview-horizontal-new.png" alt="dellma teaser" width="800" class="center">
      </div>
    </div>
  </div>


  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="column content has-text-justified">
          <p>
            MLLMs have made rapid progress in recent years, yet continue to struggle with <i>low-level visual perception</i> (LLVP)---particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robotics, medical image analysis, and manufacturing. 
            In this paper, we first introduce <i>Geoperception</i>, a benchmark designed to evaluate an MLLMâ€™s ability to accurately transcribe 2D geometric information from an image. Using this benchmark, we demonstrate the limitations of leading MLLMs, and then conduct a comprehensive empirical study to explore strategies for improving their performance on geometric tasks. Our findings highlight the benefits of certain model architectures, training techniques, and data strategies, including the use of high-fidelity synthetic data and multi-stage training with a data curriculum. Notably, we find that a data curriculum enables models to learn challenging geometry understanding tasks which they fail to learn from scratch. Leveraging these insights, we develop <span class="gradient-euclid"><b>Euclid</b></span>, a family of models specifically optimized for strong low-level geometric perception. Although purely trained on synthetic multimodal data, <span class="gradient-euclid"><b>Euclid</b></span> shows strong generalization ability to novel geometry shapes. For instance, <span class="gradient-euclid"><b>Euclid</b></span> outperforms the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on benchmark tasks and 10.65% across the tasks.            
          </p>
          
        </div>
      </div>
    </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Project Overview</h2>
        <!--<div class="content has-text-justified">-->
        <div class="column content">


<p>
We aim to study the challenges of LLVP in MLLMs, take steps to understand the root cause of their performance, and improve the models' capabilities in this area.
</p>
<p>
Our contribution consists of a complete package of evaluation benchmarks, synthetic data generation pipeline, and a family of models specifically optimized for strong low-level geometric perception.
<ol>
  <li> <b>Geoperception Benchmark</b>: We introduce a new benchmark dataset, <i>Geoperception</i>, derived from the Geometry-3K corpus, specifically designed to evaluate MLLMs' ability to accurately perceive surface-level geometric information without requiring complex inference or reasoning. Our benchmark reveals shortcomings in precise geometric perception across all leading vision-language MLLMs, both closed and open-source. </li>
  <li> <b>Empirical Study and Synthetic Data Engine</b>: To investigate the root cause of this performance, we conduct a detailed empirical exploration of MLLM architecture and training strategies. To aid in our investigation, we develop a synthetic data engine capable of generating high-fidelity visual descriptions of geometric elements. This study leads to key insights, such as the importance of certain architectural choices and the use of curriculum-based, multi-stage training with progressively more complex visual descriptions for improving low-level visual perception.  </li>
  <li> <b>Euclid Model</b>: Leveraging the insights from our exploration and our synthetic data engine, we train <span class="gradient-euclid"><b>Euclid</b></span>, a series of MLLMs tailored for high-quality geometric LLVP. Although purely trained on synthetic multimodal data with simple geometry shapes, <span class="gradient-euclid"><b>Euclid</b></span> generalizes strongly to the real-world geometry images from Geoperception benchmark, for instance, outperforming the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain benchmark tasks and 10.65% across the tasks. </li>
</ol>
</p>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered"><i>Geoperception</i> Benchmark</h2>
        <!--<div class="content has-text-justified">-->
        <div class="column content">

<p>
Geoperception is sourced from the Geometry-3K corpus, but undergoes an additional filtering process with GPT-4o-mini to ensure the presence of all geometric elements in the image. Four examples from <i>Geoperception</i> are shown below:
</p>

<p>
<img src="./static/images/geoperception_examples_main.jpg" alt="Four examples from our Geoperception dataset. The questions are sourced from the Geometry-3K corpus" width="950">
</p>

<p>
We evaluate leading open- and closed-source MLLMs, and report accuracy below. Despite the simplicity of Geoperception for humans, it remains a considerable challenge for even the most advanced commercial MLLMs. Closed-source models generally outperform open-source models.
</p>

<p>
  <img src="./static/images/geoperception_perf.png" alt="Performance of leading MLLMs on Geoperception" width="950">
</p>

</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Empirical Study on MLLM Design Space</h2>
        <!--<div class="content has-text-justified">-->
        <div class="column content">

<p>
We hypothesize the inability of today's MLLMs to effectively perceive basic geometric annotations and relationships stems from two factors: 
<ol>
  <li> The lack of high-fidelity geometric visual perception training data. </li>
  <li> The problem of their model architectures and training strategy. </li>
</ol>
Next, we will introduce our geometry dataset generation engine to overcome the lack of data, and then use generated dataset to study the optimal training strategy.
</p>

<p>
<b>Lesson 1: Under the same training dataset, scaling LLM sizes does not lead to better performance.</b>
</p>
<p>
We first vary the sizes of LLMs, Qwen-2.5~\citep{qwen2.5} in a range of 0.5B, 1.5B, and 3B while keep other components consistent. The result is shown below. We do not observe an obvious trend that larger LLMs can learn such low-level visual perception task faster or better. Moving forward, we will use Qwen-2.5-1.5B to continue our exploration.
</p>

<p>
<img src="./static/images/llm_size.jpg" alt="LLM size performance" width="950">
</p>

<p>
<b>Lesson 2: CNN architecture performs better than ViT.</b>
</p>

<p>
We then study the choice of visual encoder architectures, including two families of architectures: Vision Transformer (ViT) and ConvNeXT; as well as two visual representation learning objectives: language-supervised learning and self-supervised learning. ConvNeXT-Large shows superior learning performance with the vision transformers which are 3-5 times larger.
</p>

<p>
<img src="./static/images/encoder_comparision_main_paper.jpg" alt="encoder comparision" width="950", style="margin: 20px 0;">
</p>

<p>
<b>Lesson 3: Tuning vision encoder does not provide significant help.</b>
</p>

<p>
We next study the effect of tuning versus freezing the visual encoder. Below, we show the testing accuracy curves of tuning and freezing visual encoders. We find that compared with using a frozen encoder, tuning the visual encoder does not help the model learn low-level geometry relationships faster or better. In what follows, we will freeze the encoder for simplicity.
</p>

<p>
<img src="./static/images/freeze_unfreeze.jpg" alt="encoder tuning" width="950">
</p>

<p>
<b>Lesson 4: Curriculum learning unleashes full potential.</b>
</p>

<p>
We train the model sequentially from simple to more complex shapes and compare testing accuracy just on hard level tasks. During training, we monitor the model's performance and dynamically adjust the distribution of training data (i.e., the curriculum stage) based on this performance.
</p>

<p>
<img src="./static/images/curriculum_comparison.jpg" alt="curriculum learning" width="950">
</p>

        </div>
      </div>
    </div>
  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{liu2024dellma,
      title={{D}e{L}{L}{M}a: {D}ecision {M}aking {U}nder {U}ncertainty with {L}arge {L}anguage {M}odels},
      author={Ollie Liu$^*$ and Deqing Fu$^*$ and Dani Yogatama and Willie Neiswanger},
      year={2024},
      eprint={2402.02392},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}</code></pre>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
